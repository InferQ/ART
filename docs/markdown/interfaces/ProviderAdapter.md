[**ART Framework Component Reference**](../README.md)

***

[ART Framework Component Reference](../README.md) / ProviderAdapter

# Interface: ProviderAdapter

Defined in: [src/core/interfaces.ts:162](https://github.com/hashangit/ART/blob/4b6e07b019bda196c951a1bba064e95e97bd080e/src/core/interfaces.ts#L162)

Base interface for LLM Provider Adapters, extending the core ReasoningEngine.
Implementations will handle provider-specific API calls, authentication, etc.

## Extends

- `ReasoningEngine`

## Properties

### providerName

> `readonly` **providerName**: `string`

Defined in: [src/core/interfaces.ts:166](https://github.com/hashangit/ART/blob/4b6e07b019bda196c951a1bba064e95e97bd080e/src/core/interfaces.ts#L166)

The unique identifier name for this provider (e.g., 'openai', 'anthropic').

## Methods

### call()

> **call**(`prompt`, `options`): `Promise`\<`AsyncIterable`\<[`StreamEvent`](StreamEvent.md), `any`, `any`\>\>

Defined in: [src/core/interfaces.ts:86](https://github.com/hashangit/ART/blob/4b6e07b019bda196c951a1bba064e95e97bd080e/src/core/interfaces.ts#L86)

Executes a call to the configured Large Language Model (LLM).
This method is typically implemented by a specific `ProviderAdapter`.
When streaming is requested via `options.stream`, it returns an AsyncIterable
that yields `StreamEvent` objects as they are generated by the LLM provider.
When streaming is not requested, it should still return an AsyncIterable
that yields a minimal sequence of events (e.g., a single TOKEN event with the full response,
a METADATA event if available, and an END event).

#### Parameters

##### prompt

[`ArtStandardPrompt`](../type-aliases/ArtStandardPrompt.md)

The prompt to send to the LLM, potentially formatted specifically for the provider.

##### options

[`CallOptions`](CallOptions.md)

Options controlling the LLM call, including mandatory `threadId`, tracing IDs, model parameters (like temperature), streaming preference, and call context.

#### Returns

`Promise`\<`AsyncIterable`\<[`StreamEvent`](StreamEvent.md), `any`, `any`\>\>

A promise resolving to an AsyncIterable of `StreamEvent` objects.

#### Throws

If a critical error occurs during the initial call setup or if the stream itself errors out (typically code `LLM_PROVIDER_ERROR`).

#### Inherited from

`ReasoningEngine.call`

***

### shutdown()?

> `optional` **shutdown**(): `Promise`\<`void`\>

Defined in: [src/core/interfaces.ts:169](https://github.com/hashangit/ART/blob/4b6e07b019bda196c951a1bba064e95e97bd080e/src/core/interfaces.ts#L169)

Optional: Method for graceful shutdown

#### Returns

`Promise`\<`void`\>
