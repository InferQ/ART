[**ART Framework Component Reference**](../README.md)

***

[ART Framework Component Reference](../README.md) / ReasoningEngine

# Interface: ReasoningEngine

Defined in: [src/core/interfaces.ts:71](https://github.com/hashangit/ART/blob/9d7d0553c290c498bd29377ae972b6b43dc1b691/src/core/interfaces.ts#L71)

Interface for the component responsible for interacting with LLMs.

## Extended by

- [`ProviderAdapter`](ProviderAdapter.md)

## Methods

### call()

> **call**(`prompt`, `options`): `Promise`\<`AsyncIterable`\<[`StreamEvent`](StreamEvent.md), `any`, `any`\>\>

Defined in: [src/core/interfaces.ts:86](https://github.com/hashangit/ART/blob/9d7d0553c290c498bd29377ae972b6b43dc1b691/src/core/interfaces.ts#L86)

Executes a call to the configured Large Language Model (LLM).
This method is typically implemented by a specific `ProviderAdapter`.
When streaming is requested via `options.stream`, it returns an AsyncIterable
that yields `StreamEvent` objects as they are generated by the LLM provider.
When streaming is not requested, it should still return an AsyncIterable
that yields a minimal sequence of events (e.g., a single TOKEN event with the full response,
a METADATA event if available, and an END event).

#### Parameters

##### prompt

[`ArtStandardPrompt`](../type-aliases/ArtStandardPrompt.md)

The prompt to send to the LLM, potentially formatted specifically for the provider.

##### options

[`CallOptions`](CallOptions.md)

Options controlling the LLM call, including mandatory `threadId`, tracing IDs, model parameters (like temperature), streaming preference, and call context.

#### Returns

`Promise`\<`AsyncIterable`\<[`StreamEvent`](StreamEvent.md), `any`, `any`\>\>

A promise resolving to an AsyncIterable of `StreamEvent` objects.

#### Throws

If a critical error occurs during the initial call setup or if the stream itself errors out (typically code `LLM_PROVIDER_ERROR`).
